# -*- coding: utf-8 -*-
"""
Created on Tue May 18 16:24:58 2021

@author: Jonathas Haniel

main code and parameter to calculate flow by PINN
using tensorflow 2.5.0
visco cine	densi	visco dina		
0.00005		  0.8	     0.00004	

foward chega 7.8e-2 na sorte nos primeros 10 epochs
       2.3e-2 controlando o pinn_w com 1000+ epochs
""" 

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # or any {'0', '1', '2'}
# import sys
import tensorflow as tf
import tensorflow.keras
import numpy as np
import PINN_Dynamicweight as bib
import My_data


gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)

# Setting random seed
np.random.seed(1200)
tf.random.set_seed(1200)

# Size of Data with solution on boundary condition
N_bd_vel = 800
N_bd_out = 100
# Size of Data with solution on fluid domain
N_data_train = 100
# Number of points where weâ€™ll check physic-s informed loss, selected by Latin Hypercube (lhc)
N_lhc = 10000

# Getting the data
""" Necessary data for the mesh:
    x - x-position of each mesh elements (,1)   
    y - y-position of each mesh elements (,1)    
    t - time  of each time-step (,1)    
    X,Y,T = meshgrid(x,y,t)
    Exact u,v - Exact velocity to calcule error
    Input_star - all combination of inputs (,3)
    Output_star - all combination of outputs (,3)
    Input_train_bd - all combination of inputs used for training in boundary condition(,3)
    Output_train_bd -  all combination of outputs used for training in boundary condition(,3)
    Input_train_D - all combination of inputs used for training in fluid domain(,3)
    Output_train _D-  all combination of outputs used for training in fluid domain(,3)
  
    ub - upper boundary
    lb - lower boundary
"""
def normaliz(matr, l, u):
  return 2.0*((matr - l)/(u - l + 0.0001)) - 1.0
def denormaliz(matr, l, u):
  return ((matr+1)*(u - l + 0.0001))/2 +l


x,y,t,X,Y,Ti,Input_data_train,Output_data_train,Input_star,Output_star, \
  BD_Xvel_train,BD_Yvel_train,BD_Xpre_train,BD_Ypre_train, \
  Colocations_points,X_star,Real_output = My_data.get_data_for_test(
    N_bd_vel, N_bd_out, N_lhc,N_data_train,x_size = 260,y_size = 100, t_size = 1)

#nomalize every input data
lb = X_star.min(axis=0)
ub = X_star.max(axis=0)
x = normaliz(x,lb[0],ub[0])
X = normaliz(X,lb[0],ub[0])
y = normaliz(y,lb[1],ub[1])
Y = normaliz(Y,lb[1],ub[1])
t = normaliz(t,lb[2],ub[2])
Ti = normaliz(Ti,lb[2],ub[2])
Colocations_points = normaliz(Colocations_points,lb,ub)
BD_Xvel_train = normaliz(BD_Xvel_train,lb,ub)[:,0:2]
BD_Xpre_train = normaliz(BD_Xpre_train,lb,ub)[:,0:2]
Input_data_train = normaliz(Input_data_train,lb,ub)[:,0:2]
Input_star = normaliz(Input_star,lb,ub)[:,0:2]
X_train = np.vstack([BD_Xvel_train,BD_Xpre_train,Input_data_train])


#------------ MAIN
def error():
  u_pred= pinn.predict(Input_star)
  return np.linalg.norm(Output_star - u_pred, 2) / np.linalg.norm(Output_star, 2)
def error2():
  u_pred= pinn.predict(Input_data_train)
  return np.linalg.norm(Output_data_train - u_pred, 2) / np.linalg.norm(Output_data_train, 2)

LOGS_FILE = "test_est.csv"
WEIGHTS_PATH ='.\save\last_weights'

save_weights_flag = True
use_trained_model_flag = False
log_frequency_epoch = 10
log_frequency_batch = 5

# PIdeepNN topology (3-sized input [x y t], 9 hidden layer of 128-width, 3-sized output [u,v,p]
nodes = 16
layers = [2, nodes,nodes,nodes,nodes,nodes,nodes, 3]
# layers = [[2, nodes,nodes,nodes, 1],
#           [2, nodes,nodes,nodes, 1],
#           [2, nodes, 1]]
# layers = [2, nodes,nodes, 3]
#layers = [[2, nodes,nodes, 1],
#          [2, nodes,nodes, 1],
#          [2, nodes, 1]]

"""  *  pinn_w  =>  weight for each part separately.
order:
# momentum , mass, vel bd,pressure bd, data, out_vel

- only one that works if dynamic weight is off (is_dynamic_weight = False)
- 'influence the loss within each term, 
for example increasing the relevance of the loss by the mass conservation equation 
in relation of momentum conservation equation'

      *term_weights =>  weight for each term
order:
      [equations, boundary conditions, data]
terms are created by combining the parts:
  equation:  mass + momentum
  boundary:  vel bd + pressure bd + out_vel
  data:  data
  
- influences only if dynamic weight is on (is_dynamic_weight = True)
- 'can increase the relevance of one term o opmization search'  
"""

# Setting up the TF Adam optimizer (it can be canceled set Adam_epochs=0) RMSprop ,
Adam_epochs = 1000
batch_size = N_lhc #64
activation=tf.nn.silu
# momentum , mass, vel bd,pressure bd, data, out_vel
# pinn_w =[1,.2,20,1,20,0]
pinn_w =[1,.5,2,1,2,0]

# equations, boundary conditions, data
term_weights = [.01,.1,1]

# lr_boundaries =[N_lhc/batch_size * 10, N_lhc/batch_size *20, N_lhc/batch_size *30,N_lhc/batch_size *50,N_lhc/batch_size *300]
lr_boundaries =[N_lhc/batch_size * 1250, N_lhc/batch_size * (2500+1250)]

values = [1e-2, 3e-3, 1.1e-3]

learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(lr_boundaries, values)
# learning_rate_fn = 1e-3
Adam_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)


# ****************** PINN Navier-Stokes equations

# Creating the model and training
logs = bib.Logs(log_frequency_epoch,log_frequency_batch)
logs.set_error_fn(error)
logs.set_error_fn2(error2,'PINN_error')


pinn = bib.PINN(Adam_optimizer, logs, Colocations_points, rho = 0.8, mu = 0.00004, Steady = True)
# activation=pinn.Dynamic_activation
pinn.Foward(layers, activation) # Create pinn.model

if use_trained_model_flag:
  pinn.model.load_weights(WEIGHTS_PATH)

## INITIALIZATION:   on data points and BD only **better with DW_balance, ReLoBRalo 
                     ##     -> dont need to use with LR_Annealing(term_weight=[1,1,1] is  better)
pinn.update_dynamic_weight([0,0,1,1,10,0], 'NO')
with tf.device('/GPU:4'):
  pinn.fit(BD_Xvel_train, BD_Yvel_train, BD_Xpre_train,BD_Ypre_train,
            Input_data_train,Output_data_train, 8 , 64)
  
##  ADAM TRAINING
pinn.update_dynamic_weight(pinn_w, 'relobralo', term_weights, temperature = 1e-6, alpha_decay = 0.9, rho_std = 0.1)
with tf.device('/GPU:4'):
  pinn.fit(BD_Xvel_train, BD_Yvel_train, BD_Xpre_train,BD_Ypre_train,
            Input_data_train,Output_data_train,Adam_epochs, batch_size)
  
# ##  L-BFGS traning

# iteration = tf.Variable(0)
# def build_loss(log_frequency_LBFGS = 50):

#   # loss Function from PINN
#   loss , _ = pinn.f_loss(pinn.X_pinn_train_x, pinn.X_pinn_train_y, pinn.X_pinn_train_t, 
#               pinn.X_bd_vel, pinn.Y_bd_vel,
#               pinn.X_bd_pre, pinn.Y_bd_pre,
#               pinn.X_data,pinn.Y_data)
#   iteration.assign_add(1)
#   if tf.math.floormod(iteration, log_frequency_LBFGS) == 0:
#     tf.print('LBFGS Interation = ', iteration ,'  loss = ', loss)
#   return loss
# pinn.update_dynamic_weight([.01,.008,2,1,2,0], '', term_weights)
# ## set pinn_w = [1,1,1,1,1,1]
# trainable_variables =  pinn.model.trainable_variables  # the network weights and biases

# with tf.device('/GPU:4'):
#   bib.lbfgs_minimize(trainable_variables, build_loss,
#                       LBFGS_options = bib.set_LBFGS_options(
#                         num_correction_pairs=100, f_relative_tolerance=0, parallel_iterations = 1,
#                         tolerance=1e-5, max_iterations=1500, max_line_search_iterations=50 ))


# Getting the model predictions, from the same (x,y,t) that the predictions were previously gotten from
u_pred = pinn.predict(Input_star)
print('Final error = ', error(),'**', np.sqrt(np.square(np.mean(Output_star - u_pred)/np.mean(Output_star))))
logs.save(LOGS_FILE) #save csv file of loss and error

if save_weights_flag:
  pinn.model.save_weights(WEIGHTS_PATH, overwrite=True)

#--------end ** plot results
u_pred = pinn.predict(Input_star)
My_data.plot1(Input_star, u_pred.numpy(), X_train,Input_data_train,
             Real_output, X[:,:,0], Y[:,:,0],  x, y)


#----------------WSS
# import Shear_rate_calculation as SR
# import matplotlib.pyplot as plt
# import pickle

# with open('wall_estenose', 'rb') as f:
#   line,fit_upperbd,fit_lowerbd= pickle.load(f)

# U_pred,V_pred,P_pred = My_data.GridData(Input_star, u_pred.numpy(), X, Y)
# wall = fit_upperbd
# deltas = [2,3,4,5,6]
# resolution  = 0.0001 #m/pixel

# Shear_rate = SR.Shear_rate(U_pred,V_pred,wall,line,deltas)

# Shear_rate_avg = Shear_rate.mean(0)
# Shear_rate_max = Shear_rate.max(0)
# plt.figure()
# plt.plot(line[:,0],Shear_rate_avg,line[:,0],Shear_rate_max)
# plt.legend(['Shear_rate_avg','Shear_rate_max'])
# with open('WSS_ultimo_estenose', 'wb') as f:
#         pickle.dump([Shear_rate], f)
