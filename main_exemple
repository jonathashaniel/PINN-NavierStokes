# -*- coding: utf-8 -*-
"""
Created on Tue May 18 16:24:58 2021

@author: Jonathas Haniel

main code and parameter to calculate flow by PINN
using tensorflow 2.5.0
visco cine	densi	visco dina		
0.00005		  0.8	     0.00004	

foward chega 7.8e-2 na sorte nos primeros 10 epochs
        7e-2 controlando o pinn_w com 1000+ epochs

""" 

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # or any {'0', '1', '2'}
# import sys
import tensorflow as tf
import tensorflow.keras
import numpy as np
import PINN_LBFGS as bib
import My_data


gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)

# Setting random seed
np.random.seed(1200)
tf.random.set_seed(1200)

# Size of Data with solution on boundary condition
N_bd_vel = 800
N_bd_out = 100
# Size of Data with solution on fluid domain
N_data_train = 50
# Number of points where weâ€™ll check physic-s informed loss, selected by Latin Hypercube (lhc)
N_lhc = 1000

# Getting the data
""" Necessary data for the mesh:
    x - x-position of each mesh elements (,1)   
    y - y-position of each mesh elements (,1)    
    t - time  of each time-step (,1)    
    X,Y,T = meshgrid(x,y,t)
    Exact u,v - Exact velocity to calcule error
    Input_star - all combination of inputs (,3)
    Output_star - all combination of outputs (,3)
    Input_train_bd - all combination of inputs used for training in boundary condition(,3)
    Output_train_bd -  all combination of outputs used for training in boundary condition(,3)
    Input_train_D - all combination of inputs used for training in fluid domain(,3)
    Output_train _D-  all combination of outputs used for training in fluid domain(,3)
  
    ub - upper boundary
    lb - lower boundary
"""
def normaliz(matr, l, u):
  return 2.0*((matr - l)/(u - l + 0.0001)) - 1.0
def denormaliz(matr, l, u):
  return ((matr+1)*(u - l + 0.0001))/2 +l


x,y,t,X,Y,Ti,Input_data_train,Output_data_train,Input_star,Output_star, \
  BD_Xvel_train,BD_Yvel_train,BD_Xpre_train,BD_Ypre_train, \
  Colocations_points,X_star,Real_output = My_data.get_data_for_test(
    N_bd_vel, N_bd_out, N_lhc,N_data_train,x_size = 260,y_size = 100, t_size = 1)

#nomalize every input data
lb = X_star.min(axis=0)
ub = X_star.max(axis=0)
x = normaliz(x,lb[0],ub[0])
X = normaliz(X,lb[0],ub[0])
y = normaliz(y,lb[1],ub[1])
Y = normaliz(Y,lb[1],ub[1])
t = normaliz(t,lb[2],ub[2])
Ti = normaliz(Ti,lb[2],ub[2])
Colocations_points = normaliz(Colocations_points,lb,ub)
BD_Xvel_train = normaliz(BD_Xvel_train,lb,ub)[:,0:2]
BD_Xpre_train = normaliz(BD_Xpre_train,lb,ub)[:,0:2]
Input_data_train = normaliz(Input_data_train,lb,ub)[:,0:2]
Input_star = normaliz(Input_star,lb,ub)[:,0:2]



#------------ MAIN
def error():
  u_pred= pinn.predict(Input_star)
  return np.linalg.norm(Output_star - u_pred, 2) / np.linalg.norm(Output_star, 2)
def error2():
  u_pred= pinn.predict(Input_data_train)
  return np.linalg.norm(Output_data_train - u_pred, 2) / np.linalg.norm(Output_data_train, 2)

LOGS_FILE = "test_est.csv"
WEIGHTS_PATH ='.\save\last_weights'

save_weights_flag = False
use_trained_model_flag = True
log_frequency_epoch = 1
log_frequency_batch = 10

# PIdeepNN topology (3-sized input [x y t], 9 hidden layer of 128-width, 3-sized output [u,v,p]
nodes = 16
layers = [2, nodes,nodes,nodes,nodes,nodes,nodes, 3]
# layers = [[2, nodes,nodes,nodes,nodes,nodes,nodes,nodes,nodes, 1],
#           [2, nodes,nodes,nodes,nodes,nodes,nodes,nodes,nodes, 1],
#           [2, nodes,nodes,nodes,nodes,nodes,nodes,nodes, 1]]
# layers = [2, nodes,nodes,nodes,3]
# layers = [[2, nodes,nodes,nodes,1],
#           [2, nodes,nodes,nodes,1],
#           [2, nodes,1]]

activation=tf.nn.silu
# Setting up the TF Adam optimizer (it can be canceled set Adam_epochs=0) RMSprop ,
Adam_epochs = 0
batch_size = 64
# momentum , mass, vel bd,pressure bd, data, out_vel
# pinn_w =[1,.2,20,1,20,0]
pinn_w =[2,.2,20,1,20,0]

lr_boundaries =[N_lhc/batch_size * 5, N_lhc/batch_size *15, N_lhc/batch_size *30,N_lhc/batch_size * 70,N_lhc/batch_size *300]
values = [ 1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 1e-4]
learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(lr_boundaries, values)


# Adam_optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate_fn)
Adam_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)

# ****************** PINN Navier-Stokes equations

# Creating the model and training
logs = bib.Logs(log_frequency_epoch,log_frequency_batch)
logs.set_error_fn(error)
logs.set_error_fn2(error2,'PINN_error')

pinn = bib.PINN(Adam_optimizer, logs, Colocations_points, pinn_w, rho = 0.8, mu = 0.00004, Steady = True)
pinn.Foward(layers,activation) # Create pinn.model

if use_trained_model_flag:
  pinn.model.load_weights(WEIGHTS_PATH)
  
with tf.device('/GPU:5'):
  pinn.fit(BD_Xvel_train, BD_Yvel_train, BD_Xpre_train,BD_Ypre_train,
            Input_data_train,Output_data_train,Adam_epochs, batch_size)

###  L-BFGS traing
iteration = tf.Variable(0)
def build_loss(log_frequency_LBFGS = 50):

  # loss Function from PINN
  loss , _ = pinn.f_loss(pinn.X_pinn_train_x, pinn.X_pinn_train_y, pinn.X_pinn_train_t, 
             pinn.X_bd_vel, pinn.Y_bd_vel,
             pinn.X_bd_pre, pinn.Y_bd_pre,
             pinn.X_data,pinn.Y_data)
  iteration.assign_add(1)
  if tf.math.floormod(iteration, log_frequency_LBFGS) == 0:
    tf.print('LBFGS Interation = ', iteration ,'  loss = ', loss)
  return loss

trainable_variables =  pinn.model.trainable_variables  # the network weights and biases
with tf.device('/GPU:5'):
  bib.lbfgs_minimize(trainable_variables, build_loss,
                     LBFGS_options = bib.set_LBFGS_options(
                       num_correction_pairs=100, f_relative_tolerance=0, parallel_iterations = 1,
                       tolerance=1e-6, max_iterations=15000, max_line_search_iterations=50 ))

print('Final error = ', error())
# logs.save(LOGS_FILE) #save csv file of loss and error

# Getting the model predictions, from the same (x,y,t) that the predictions were previously gotten from
u_pred = pinn.predict(Input_star)

if save_weights_flag:
  pinn.model.save_weights(WEIGHTS_PATH, overwrite=True)



#--------end ** plot results
X_train = np.vstack([BD_Xvel_train,BD_Xpre_train,Input_data_train])
My_data.plot1(Input_star, u_pred.numpy(), X_train,Input_data_train,
             Real_output, X[:,:,0], Y[:,:,0],  x, y)


